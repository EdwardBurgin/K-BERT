{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "useage: [--pretrained_model_path] - Path to the pre-trained model parameters.\n",
    "        [--config_path] - Path to the model configuration file.\n",
    "        [--vocab_path] - Path to the vocabulary file.\n",
    "        --train_path - Path to the training dataset.\n",
    "        --dev_path - Path to the validating dataset.\n",
    "        --test_path - Path to the testing dataset.\n",
    "        [--epochs_num] - The number of training epoches.\n",
    "        [--batch_size] - Batch size of the training process.\n",
    "        [--kg_name] - The name of knowledge graph, \"HowNet\", \"CnDbpedia\" or \"Medical\".\n",
    "        [--output_model_path] - Path to the output model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- encoding:utf-8 -*-\n",
    "\"\"\"\n",
    "  This script provides an k-BERT exmaple for classification.\n",
    "\"\"\"\n",
    "import sys\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "import collections\n",
    "import torch.nn as nn\n",
    "from uer.utils.vocab import Vocab\n",
    "from uer.utils.constants import *\n",
    "from uer.utils.tokenizer import * \n",
    "from uer.model_builder import build_model\n",
    "from uer.utils.optimizers import  BertAdam\n",
    "from uer.utils.config import load_hyperparam  \n",
    "from uer.utils.seed import set_seed\n",
    "from uer.model_saver import save_model\n",
    "from multiprocessing import Process, Pool\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import inspect\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_chinese = pd.read_csv('../../NLP_DATA/multilingual_data/200410_train_stratshuf_chinese.csv')\n",
    "# df_test_chinese = pd.read_csv('../../NLP_DATA/multilingual_data/200410_test_stratshuf_chinese.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_chinese.rename({'text':'text_a','labels':'label'}, axis=1, inplace=True)\n",
    "# df_train_chinese[['text_a', 'label']].to_csv('huawei_train.csv', sep='\\t',index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test_chinese.rename({'text':'text_a','labels':'label'}, axis=1, inplace=True)\n",
    "# df_test_chinese[['text_a', 'label']].to_csv('huawei_test.csv', sep='\\t',index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class huawei():\n",
    "    def __init__(self):\n",
    "        self.pretrained_model_path='./models/google_model.bin'\n",
    "        self.output_model_path=\"./models/classifier_model.bin\"\n",
    "        self.vocab_path=\"./models/google_vocab.txt\"\n",
    "        self.train_path='./huawei_train.csv'\n",
    "        self.config_path = './models/google_config.json'\n",
    "        self.vocab_path = './models/google_vocab.txt'\n",
    "        self.dev_path = './huawei_test.csv'\n",
    "        self.test_path ='./huawei_test.csv'\n",
    "        self.epochs_num = 35\n",
    "        self.batch_size =  30\n",
    "        self.kg_name = 'CnDbpedia'#'short_CnDbpedia'\n",
    "        self.output_model_path= './outputs/kbert_huawei_CnDbpedia.bin'\n",
    "        self.pooling='last'#'first'\n",
    "\n",
    "        self.seq_length=140\n",
    "        self.encoder=\"bert\"\n",
    "        self.bidirectional=True #\"store_true\"\n",
    "\n",
    "        # Subword options.\n",
    "        self.subword_type= 'none'#\"char\"#'none'\n",
    "        self.sub_vocab_path = \"models/sub_vocab.txt\"\n",
    "        self.subencoder=\"avg\" #choices=[\"avg\", \"lstm\", \"gru\", \"cnn\"] #sota avg\n",
    "        self.sub_layers_num=2\n",
    "\n",
    "        # Tokenizer options.\n",
    "        self.tokenizer=\"bert\"\n",
    "            \n",
    "\n",
    "        # Optimizer options.\n",
    "        self.learning_rate =2e-5\n",
    "                   \n",
    "        self.warmup=0.1\n",
    "                \n",
    "\n",
    "        # Training options.\n",
    "        self.dropout=default=0.5\n",
    "\n",
    "                   \n",
    "        self.report_steps=100\n",
    "        self.seed=7\n",
    "        # Evaluation options.\n",
    "        self.mean_reciprocal_rank=False#'store_true'\n",
    "        # kg\n",
    "        self.workers_num=1#, help=\"number of process for loading dataset\")\n",
    "        self.no_vm= False #'store_true'#, help=\"Disable the visible_matrix\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# Load the hyperparameters from the config file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = argsp()\n",
    "args=huawei()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = load_hyperparam(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__class__', __main__.huawei),\n",
       " ('__delattr__',\n",
       "  <method-wrapper '__delattr__' of huawei object at 0x7f55cc4b0588>),\n",
       " ('__dict__',\n",
       "  {'pretrained_model_path': './models/google_model.bin',\n",
       "   'output_model_path': './outputs/kbert_huawei_CnDbpedia.bin',\n",
       "   'vocab_path': './models/google_vocab.txt',\n",
       "   'train_path': './huawei_train.csv',\n",
       "   'config_path': './models/google_config.json',\n",
       "   'dev_path': './huawei_test.csv',\n",
       "   'test_path': './huawei_test.csv',\n",
       "   'epochs_num': 35,\n",
       "   'batch_size': 30,\n",
       "   'kg_name': 'CnDbpedia',\n",
       "   'pooling': 'last',\n",
       "   'seq_length': 140,\n",
       "   'encoder': 'bert',\n",
       "   'bidirectional': True,\n",
       "   'subword_type': 'none',\n",
       "   'sub_vocab_path': 'models/sub_vocab.txt',\n",
       "   'subencoder': 'avg',\n",
       "   'sub_layers_num': 2,\n",
       "   'tokenizer': 'bert',\n",
       "   'learning_rate': 2e-05,\n",
       "   'warmup': 0.1,\n",
       "   'dropout': 0.1,\n",
       "   'report_steps': 100,\n",
       "   'seed': 7,\n",
       "   'mean_reciprocal_rank': False,\n",
       "   'workers_num': 1,\n",
       "   'no_vm': False,\n",
       "   'emb_size': 768,\n",
       "   'hidden_size': 768,\n",
       "   'kernel_size': 3,\n",
       "   'block_size': 2,\n",
       "   'feedforward_size': 3072,\n",
       "   'heads_num': 12,\n",
       "   'layers_num': 12}),\n",
       " ('__dir__', <function huawei.__dir__>),\n",
       " ('__doc__', None),\n",
       " ('__eq__', <method-wrapper '__eq__' of huawei object at 0x7f55cc4b0588>),\n",
       " ('__format__', <function huawei.__format__>),\n",
       " ('__ge__', <method-wrapper '__ge__' of huawei object at 0x7f55cc4b0588>),\n",
       " ('__getattribute__',\n",
       "  <method-wrapper '__getattribute__' of huawei object at 0x7f55cc4b0588>),\n",
       " ('__gt__', <method-wrapper '__gt__' of huawei object at 0x7f55cc4b0588>),\n",
       " ('__hash__', <method-wrapper '__hash__' of huawei object at 0x7f55cc4b0588>),\n",
       " ('__init__',\n",
       "  <bound method huawei.__init__ of <__main__.huawei object at 0x7f55cc4b0588>>),\n",
       " ('__init_subclass__', <function huawei.__init_subclass__>),\n",
       " ('__le__', <method-wrapper '__le__' of huawei object at 0x7f55cc4b0588>),\n",
       " ('__lt__', <method-wrapper '__lt__' of huawei object at 0x7f55cc4b0588>),\n",
       " ('__module__', '__main__'),\n",
       " ('__ne__', <method-wrapper '__ne__' of huawei object at 0x7f55cc4b0588>),\n",
       " ('__new__', <function object.__new__(*args, **kwargs)>),\n",
       " ('__reduce__', <function huawei.__reduce__>),\n",
       " ('__reduce_ex__', <function huawei.__reduce_ex__>),\n",
       " ('__repr__', <method-wrapper '__repr__' of huawei object at 0x7f55cc4b0588>),\n",
       " ('__setattr__',\n",
       "  <method-wrapper '__setattr__' of huawei object at 0x7f55cc4b0588>),\n",
       " ('__sizeof__', <function huawei.__sizeof__>),\n",
       " ('__str__', <method-wrapper '__str__' of huawei object at 0x7f55cc4b0588>),\n",
       " ('__subclasshook__', <function huawei.__subclasshook__>),\n",
       " ('__weakref__', None),\n",
       " ('batch_size', 30),\n",
       " ('bidirectional', True),\n",
       " ('block_size', 2),\n",
       " ('config_path', './models/google_config.json'),\n",
       " ('dev_path', './huawei_test.csv'),\n",
       " ('dropout', 0.1),\n",
       " ('emb_size', 768),\n",
       " ('encoder', 'bert'),\n",
       " ('epochs_num', 35),\n",
       " ('feedforward_size', 3072),\n",
       " ('heads_num', 12),\n",
       " ('hidden_size', 768),\n",
       " ('kernel_size', 3),\n",
       " ('kg_name', 'CnDbpedia'),\n",
       " ('layers_num', 12),\n",
       " ('learning_rate', 2e-05),\n",
       " ('mean_reciprocal_rank', False),\n",
       " ('no_vm', False),\n",
       " ('output_model_path', './outputs/kbert_huawei_CnDbpedia.bin'),\n",
       " ('pooling', 'last'),\n",
       " ('pretrained_model_path', './models/google_model.bin'),\n",
       " ('report_steps', 100),\n",
       " ('seed', 7),\n",
       " ('seq_length', 140),\n",
       " ('sub_layers_num', 2),\n",
       " ('sub_vocab_path', 'models/sub_vocab.txt'),\n",
       " ('subencoder', 'avg'),\n",
       " ('subword_type', 'none'),\n",
       " ('test_path', './huawei_test.csv'),\n",
       " ('tokenizer', 'bert'),\n",
       " ('train_path', './huawei_train.csv'),\n",
       " ('vocab_path', './models/google_vocab.txt'),\n",
       " ('warmup', 0.1),\n",
       " ('workers_num', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getmembers(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE_DIR_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# KGS = {\n",
    "#     'HowNet': os.path.join(FILE_DIR_PATH, 'kgs/HowNet.spo'),\n",
    "#     'CnDbpedia': os.path.join(FILE_DIR_PATH, 'kgs/CnDbpedia.spo'),\n",
    "#     'Medical': os.path.join(FILE_DIR_PATH, 'kgs/Medical.spo'),\n",
    "# }\n",
    "\n",
    "# MAX_ENTITIES = 2\n",
    "\n",
    "# # Special token words.\n",
    "# PAD_TOKEN = '[PAD]'\n",
    "# UNK_TOKEN = '[UNK]'\n",
    "# CLS_TOKEN = '[CLS]'\n",
    "# SEP_TOKEN = '[SEP]'\n",
    "# MASK_TOKEN = '[MASK]'\n",
    "# ENT_TOKEN = '[ENT]'\n",
    "# SUB_TOKEN = '[SUB]'\n",
    "# PRE_TOKEN = '[PRE]'\n",
    "# OBJ_TOKEN = '[OBJ]'\n",
    "\n",
    "# NEVER_SPLIT_TAG = [\n",
    "#     PAD_TOKEN, UNK_TOKEN, CLS_TOKEN, SEP_TOKEN, MASK_TOKEN,\n",
    "#     ENT_TOKEN, SUB_TOKEN, PRE_TOKEN, OBJ_TOKEN\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, './brain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\"\"\"\n",
    "KnowledgeGraph\n",
    "\"\"\"\n",
    "import os\n",
    "import config\n",
    "import pkuseg\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from knowgraph import KnowledgeGraph\n",
    "class KnowledgeGraph(object):\n",
    "    \"\"\"\n",
    "    spo_files - list of Path of *.spo files, or default kg name. e.g., ['HowNet']\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, spo_files, predicate=False):\n",
    "        self.predicate = predicate\n",
    "        self.spo_file_paths = [config.KGS.get(f, f) for f in spo_files]\n",
    "        self.lookup_table = self._create_lookup_table()\n",
    "        print('extracting keys')\n",
    "        self.segment_vocab = list(self.lookup_table.keys()) + config.NEVER_SPLIT_TAG\n",
    "        print('tokenize')\n",
    "        self.tokenizer = pkuseg.pkuseg(model_name=\"default\", postag=False, user_dict=self.segment_vocab) #just initialise\n",
    "        print('end tokenize')\n",
    "        self.special_tags = set(config.NEVER_SPLIT_TAG)\n",
    "        self.entities = {}\n",
    "\n",
    "    def _create_lookup_table(self):\n",
    "        lookup_table = {}\n",
    "        for spo_path in self.spo_file_paths:\n",
    "            print(\"[KnowledgeGraph] Loading spo from {}\".format(spo_path))\n",
    "            with open(spo_path, 'r', encoding='utf-8') as f:\n",
    "                for line in tqdm(f):\n",
    "                    try:\n",
    "                        subj, pred, obje = line.strip().split(\"\\t\")    \n",
    "                    except:\n",
    "                        print(\"[KnowledgeGraph] Bad spo:\", line)\n",
    "                    if self.predicate:\n",
    "                        value = pred + obje\n",
    "                    else:\n",
    "                        value = obje\n",
    "                    if subj in lookup_table.keys():\n",
    "                        lookup_table[subj].add(value)\n",
    "                    else:\n",
    "                        lookup_table[subj] = set([value])\n",
    "        return lookup_table\n",
    "\n",
    "    def add_knowledge_with_vm(self, sent_batch, max_entities=config.MAX_ENTITIES, add_pad=True, max_length=128, debug = False):\n",
    "        \"\"\"\n",
    "        input: sent_batch - list of sentences, e.g., [\"abcd\", \"efgh\"]\n",
    "        entities is the set of detected entities in the train.\n",
    "        return: know_sent_batch - list of sentences with entities embedding\n",
    "                position_batch - list of position index of each character.\n",
    "                visible_matrix_batch - list of visible matrixs\n",
    "                seg_batch - list of segment tags\n",
    "        \"\"\"\n",
    "        split_sent_batch = [self.tokenizer.cut(sent) for sent in sent_batch]\n",
    "        know_sent_batch = []\n",
    "        position_batch = []\n",
    "        visible_matrix_batch = []\n",
    "        seg_batch = []\n",
    "        for split_sent in split_sent_batch:\n",
    "\n",
    "            # create tree\n",
    "            sent_tree = []\n",
    "            pos_idx_tree = []\n",
    "            abs_idx_tree = []\n",
    "            pos_idx = -1\n",
    "            abs_idx = -1\n",
    "            abs_idx_src = []\n",
    "            for token in split_sent:\n",
    "\n",
    "                entities = list(self.lookup_table.get(token, []))[:max_entities]\n",
    "                if debug: \n",
    "                    print(entities)  ######\n",
    "                if token in self.entities.keys():\n",
    "                    self.entities[token].extend(entities)\n",
    "                else:\n",
    "                    self.entities[token]=entities\n",
    "                    \n",
    "                sent_tree.append((token, entities))\n",
    "\n",
    "                if token in self.special_tags:\n",
    "                    token_pos_idx = [pos_idx+1]\n",
    "                    token_abs_idx = [abs_idx+1]\n",
    "                else:\n",
    "                    token_pos_idx = [pos_idx+i for i in range(1, len(token)+1)]\n",
    "                    token_abs_idx = [abs_idx+i for i in range(1, len(token)+1)]\n",
    "                abs_idx = token_abs_idx[-1]\n",
    "\n",
    "                entities_pos_idx = []\n",
    "                entities_abs_idx = []\n",
    "                for ent in entities:\n",
    "                    ent_pos_idx = [token_pos_idx[-1] + i for i in range(1, len(ent)+1)]\n",
    "                    entities_pos_idx.append(ent_pos_idx)\n",
    "                    ent_abs_idx = [abs_idx + i for i in range(1, len(ent)+1)]\n",
    "                    abs_idx = ent_abs_idx[-1]\n",
    "                    entities_abs_idx.append(ent_abs_idx)\n",
    "\n",
    "                pos_idx_tree.append((token_pos_idx, entities_pos_idx))\n",
    "                pos_idx = token_pos_idx[-1]\n",
    "                abs_idx_tree.append((token_abs_idx, entities_abs_idx))\n",
    "                abs_idx_src += token_abs_idx\n",
    "\n",
    "            # Get know_sent and pos\n",
    "            know_sent = []\n",
    "            pos = []\n",
    "            seg = []\n",
    "            for i in range(len(sent_tree)):\n",
    "                word = sent_tree[i][0]\n",
    "                if word in self.special_tags:\n",
    "                    know_sent += [word]\n",
    "                    seg += [0]\n",
    "                else:\n",
    "                    add_word = list(word)\n",
    "                    know_sent += add_word \n",
    "                    seg += [0] * len(add_word)\n",
    "                pos += pos_idx_tree[i][0]\n",
    "                for j in range(len(sent_tree[i][1])):\n",
    "                    add_word = list(sent_tree[i][1][j])\n",
    "                    know_sent += add_word\n",
    "                    seg += [1] * len(add_word)\n",
    "                    pos += list(pos_idx_tree[i][1][j])\n",
    "\n",
    "            token_num = len(know_sent)\n",
    "\n",
    "            # Calculate visible matrix\n",
    "            visible_matrix = np.zeros((token_num, token_num))\n",
    "            for item in abs_idx_tree:\n",
    "                src_ids = item[0]\n",
    "                for id in src_ids:\n",
    "                    visible_abs_idx = abs_idx_src + [idx for ent in item[1] for idx in ent]\n",
    "                    visible_matrix[id, visible_abs_idx] = 1\n",
    "                for ent in item[1]:\n",
    "                    for id in ent:\n",
    "                        visible_abs_idx = ent + src_ids\n",
    "                        visible_matrix[id, visible_abs_idx] = 1\n",
    "\n",
    "            src_length = len(know_sent)\n",
    "            if len(know_sent) < max_length:\n",
    "                pad_num = max_length - src_length\n",
    "                know_sent += [config.PAD_TOKEN] * pad_num\n",
    "                seg += [0] * pad_num\n",
    "                pos += [max_length - 1] * pad_num\n",
    "                visible_matrix = np.pad(visible_matrix, ((0, pad_num), (0, pad_num)), 'constant')  # pad 0\n",
    "            else:\n",
    "                know_sent = know_sent[:max_length]\n",
    "                seg = seg[:max_length]\n",
    "                pos = pos[:max_length]\n",
    "                visible_matrix = visible_matrix[:max_length, :max_length]\n",
    "            \n",
    "            know_sent_batch.append(know_sent)\n",
    "            position_batch.append(pos)\n",
    "            visible_matrix_batch.append(visible_matrix)\n",
    "            seg_batch.append(seg)\n",
    "        \n",
    "        return know_sent_batch, position_batch, visible_matrix_batch, seg_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom BERT wrapper.\n",
    "    \"\"\"\n",
    "    def __init__(self, args, model):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.embedding = model.embedding\n",
    "        self.encoder = model.encoder\n",
    "        self.labels_num = args.labels_num\n",
    "        self.pooling = args.pooling\n",
    "        self.output_layer_1 = nn.Linear(args.hidden_size, args.hidden_size)\n",
    "        self.output_layer_2 = nn.Linear(args.hidden_size, args.labels_num)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        self.criterion = nn.NLLLoss()\n",
    "        self.use_vm = False if args.no_vm else True\n",
    "        print(\"[BertClassifier] use visible_matrix: {}\".format(self.use_vm))\n",
    "\n",
    "    def forward(self, src, label, mask, pos=None, vm=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: [batch_size x seq_length]\n",
    "            label: [batch_size]\n",
    "            mask: [batch_size x seq_length]\n",
    "        \"\"\"\n",
    "        # Embedding.\n",
    "        emb = self.embedding(src, mask, pos)\n",
    "        # Encoder.\n",
    "        if not self.use_vm:\n",
    "            vm = None\n",
    "        output = self.encoder(emb, mask, vm)\n",
    "        # Target.\n",
    "        if self.pooling == \"mean\":\n",
    "            output = torch.mean(output, dim=1)\n",
    "        elif self.pooling == \"max\":\n",
    "            output = torch.max(output, dim=1)[0]\n",
    "        elif self.pooling == \"last\":\n",
    "            output = output[:, -1, :]\n",
    "        else:\n",
    "            output = output[:, 0, :]\n",
    "        output = torch.tanh(self.output_layer_1(output))\n",
    "        logits = self.output_layer_2(output)\n",
    "        loss = self.criterion(self.softmax(logits.view(-1, self.labels_num)), label.view(-1))\n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary file line 344 has bad format token\n",
      "Vocabulary Size:  21128\n",
      "[BertClassifier] use visible_matrix: True\n"
     ]
    }
   ],
   "source": [
    "set_seed(args.seed)\n",
    "\n",
    "# Count the number of labels.\n",
    "labels_set = set()\n",
    "columns = {}\n",
    "with open(args.train_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    for line_id, line in enumerate(f):\n",
    "        try:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            if line_id == 0:\n",
    "                for i, column_name in enumerate(line):\n",
    "                    columns[column_name] = i\n",
    "                continue\n",
    "            label = int(line[columns[\"label\"]])\n",
    "            labels_set.add(label)\n",
    "        except:\n",
    "            pass\n",
    "args.labels_num = len(labels_set) \n",
    "\n",
    "# Load vocabulary.\n",
    "vocab = Vocab()\n",
    "vocab.load(args.vocab_path)\n",
    "args.vocab = vocab\n",
    "\n",
    "# Build bert model.\n",
    "# A pseudo target is added.\n",
    "args.target = \"bert\"\n",
    "model = build_model(args)  ##USES UER FRAMEWORK\n",
    "\n",
    "# Load or initialize parameters.\n",
    "if args.pretrained_model_path is not None:\n",
    "    # Initialize with pretrained model.\n",
    "    model.load_state_dict(torch.load(args.pretrained_model_path), strict=False)  \n",
    "else:\n",
    "    # Initialize with normal distribution.\n",
    "    for n, p in list(model.named_parameters()):\n",
    "        if 'gamma' not in n and 'beta' not in n:\n",
    "            p.data.normal_(0, 0.02)\n",
    "\n",
    "# Build classification model.\n",
    "model = BertClassifier(args, model)\n",
    "\n",
    "# For simplicity, we use DataParallel wrapper to use multiple GPUs.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"{} GPUs are available. Let's use them.\".format(torch.cuda.device_count()))\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.labels_num = len(labels_set) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "336"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.labels_num "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batch_loader(batch_size, input_ids, label_ids, mask_ids, pos_ids, vms):\n",
    "    \"\"\"\n",
    "    Just takes inputs, labels, masks, positions and VM to create batch. Nothing more.\n",
    "    \"\"\"\n",
    "    instances_num = input_ids.size()[0]\n",
    "    for i in range(instances_num // batch_size):\n",
    "        input_ids_batch = input_ids[i*batch_size: (i+1)*batch_size, :]\n",
    "        label_ids_batch = label_ids[i*batch_size: (i+1)*batch_size]\n",
    "        mask_ids_batch = mask_ids[i*batch_size: (i+1)*batch_size, :]\n",
    "        pos_ids_batch = pos_ids[i*batch_size: (i+1)*batch_size, :]\n",
    "        vms_batch = vms[i*batch_size: (i+1)*batch_size]\n",
    "        yield input_ids_batch, label_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch\n",
    "    if instances_num > instances_num // batch_size * batch_size:\n",
    "        input_ids_batch = input_ids[instances_num//batch_size*batch_size:, :]\n",
    "        label_ids_batch = label_ids[instances_num//batch_size*batch_size:]\n",
    "        mask_ids_batch = mask_ids[instances_num//batch_size*batch_size:, :]\n",
    "        pos_ids_batch = pos_ids[instances_num//batch_size*batch_size:, :]\n",
    "        vms_batch = vms[instances_num//batch_size*batch_size:]\n",
    "\n",
    "        yield input_ids_batch, label_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch\n",
    "\n",
    "# Build knowledge graph.\n",
    "if args.kg_name == 'none':\n",
    "    spo_files = []\n",
    "else:\n",
    "    spo_files = [args.kg_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41920it [00:00, 419197.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KnowledgeGraph] Loading spo from /home/aiops/EDWARD/nlp_code/K-BERT/brain/kgs/CnDbpedia.spo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5168865it [00:08, 591507.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting keys\n",
      "tokenize\n",
      "end tokenize\n"
     ]
    }
   ],
   "source": [
    "kg = KnowledgeGraph(spo_files=spo_files, predicate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect.getmembers(kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHOW THE KNOWLEDGE\n",
    "# lim = {k:set(kg.entities[k]) for  k in kg.entities.keys() if len(set(kg.entities[k]))>0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_knowledge_worker(params, debug=False):\n",
    "    '''\n",
    "    This determines if it is a sentence classification or similarity of 2 sentences type task.\n",
    "    '''\n",
    "    p_id, sentences, columns, kg, vocab, args = params\n",
    "\n",
    "    sentences_num = len(sentences)\n",
    "    dataset = []\n",
    "    for line_id, line in enumerate(sentences):\n",
    "        if line_id % 10000 == 0:\n",
    "            print(\"Progress of process {}: {}/{}\".format(p_id, line_id, sentences_num))\n",
    "            sys.stdout.flush()\n",
    "        line = line.strip().split('\\t')\n",
    "        try:\n",
    "            if len(line) == 2:\n",
    "                if debug: print(columns, line) ########\n",
    "                label = int(line[columns[\"label\"]])\n",
    "                text = CLS_TOKEN + line[columns[\"text_a\"]]\n",
    "   \n",
    "                tokens, pos, vm, _ = kg.add_knowledge_with_vm([text], add_pad=True, max_length=args.seq_length)\n",
    "                tokens = tokens[0]\n",
    "                pos = pos[0]\n",
    "                vm = vm[0].astype(\"bool\")\n",
    "                if debug: print(tokens) #######\n",
    "                token_ids = [vocab.get(t) for t in tokens]\n",
    "                mask = [1 if t != PAD_TOKEN else 0 for t in tokens]\n",
    "\n",
    "                dataset.append((token_ids, label, mask, pos, vm))\n",
    "            \n",
    "            elif len(line) == 3:\n",
    "                label = int(line[columns[\"label\"]])\n",
    "                text = CLS_TOKEN + line[columns[\"text_a\"]] + SEP_TOKEN + line[columns[\"text_b\"]] + SEP_TOKEN\n",
    "\n",
    "                tokens, pos, vm, _ = kg.add_knowledge_with_vm([text], add_pad=True, max_length=args.seq_length)\n",
    "                tokens = tokens[0]\n",
    "                pos = pos[0]\n",
    "                vm = vm[0].astype(\"bool\")\n",
    "\n",
    "                token_ids = [vocab.get(t) for t in tokens]\n",
    "                mask = []\n",
    "                seg_tag = 1\n",
    "                for t in tokens:\n",
    "                    if t == PAD_TOKEN:\n",
    "                        mask.append(0)\n",
    "                    else:\n",
    "                        mask.append(seg_tag)\n",
    "                    if t == SEP_TOKEN:\n",
    "                        seg_tag += 1\n",
    "\n",
    "                dataset.append((token_ids, label, mask, pos, vm))\n",
    "            \n",
    "            elif len(line) == 4:  # for dbqa\n",
    "                qid=int(line[columns[\"qid\"]])\n",
    "                label = int(line[columns[\"label\"]])\n",
    "                text_a, text_b = line[columns[\"text_a\"]], line[columns[\"text_b\"]]\n",
    "                text = CLS_TOKEN + text_a + SEP_TOKEN + text_b + SEP_TOKEN\n",
    "\n",
    "                tokens, pos, vm, _ = kg.add_knowledge_with_vm([text], add_pad=True, max_length=args.seq_length)\n",
    "                tokens = tokens[0]\n",
    "                pos = pos[0]\n",
    "                vm = vm[0].astype(\"bool\")\n",
    "\n",
    "                token_ids = [vocab.get(t) for t in tokens]\n",
    "                mask = []\n",
    "                seg_tag = 1\n",
    "                for t in tokens:\n",
    "                    if t == PAD_TOKEN:\n",
    "                        mask.append(0)\n",
    "                    else:\n",
    "                        mask.append(seg_tag)\n",
    "                    if t == SEP_TOKEN:\n",
    "                        seg_tag += 1\n",
    "                \n",
    "                dataset.append((token_ids, label, mask, pos, vm, qid))\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"Error line: \", line, e)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_dataset(path, workers_num=1):\n",
    "    \"\"\"\n",
    "    runs add_knowledge_worker on dataset\n",
    "    \"\"\"\n",
    "    print(\"Loading sentences from {}\".format(path))\n",
    "    sentences = []\n",
    "    with open(path, mode='r', encoding=\"utf-8\") as f:\n",
    "        for line_id, line in enumerate(f):\n",
    "            if line_id == 0:\n",
    "                continue\n",
    "            sentences.append(line)\n",
    "    sentence_num = len(sentences)\n",
    "\n",
    "    print(\"There are {} sentence in total. We use {} processes to inject knowledge into sentences.\".format(sentence_num, workers_num))\n",
    "    if workers_num > 1:\n",
    "        params = []\n",
    "        sentence_per_block = int(sentence_num / workers_num) + 1\n",
    "        for i in range(workers_num):\n",
    "            params.append((i, sentences[i*sentence_per_block: (i+1)*sentence_per_block], columns, kg, vocab, args))\n",
    "        pool = Pool(workers_num)\n",
    "        res = pool.map(add_knowledge_worker, params)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        dataset = [sample for block in res for sample in block]\n",
    "    else:\n",
    "        params = (0, sentences, columns, kg, vocab, args)\n",
    "        dataset = add_knowledge_worker(params)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Evaluation function.\n",
    "def evaluate(args, is_test, metrics='Acc'):\n",
    "    if is_test:\n",
    "        dataset = read_dataset(args.test_path, workers_num=args.workers_num)\n",
    "    else:\n",
    "        dataset = read_dataset(args.dev_path, workers_num=args.workers_num)\n",
    "\n",
    "    input_ids = torch.LongTensor([sample[0] for sample in dataset])\n",
    "    label_ids = torch.LongTensor([sample[1] for sample in dataset])\n",
    "    mask_ids = torch.LongTensor([sample[2] for sample in dataset])\n",
    "    pos_ids = torch.LongTensor([example[3] for example in dataset])\n",
    "    vms = [example[4] for example in dataset]\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "    instances_num = input_ids.size()[0]\n",
    "    if is_test:\n",
    "        print(\"The number of evaluation instances: \", instances_num)\n",
    "\n",
    "    correct = 0\n",
    "    # Confusion matrix.\n",
    "    confusion = torch.zeros(args.labels_num, args.labels_num, dtype=torch.long)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    if not args.mean_reciprocal_rank:\n",
    "        print('not MMR')\n",
    "        for i, (input_ids_batch, label_ids_batch,  mask_ids_batch, pos_ids_batch, vms_batch) in enumerate(batch_loader(batch_size, input_ids, label_ids, mask_ids, pos_ids, vms)):\n",
    "\n",
    "            # vms_batch = vms_batch.long()\n",
    "            vms_batch = torch.LongTensor(vms_batch)\n",
    "\n",
    "            input_ids_batch = input_ids_batch.to(device)\n",
    "            label_ids_batch = label_ids_batch.to(device)\n",
    "            mask_ids_batch = mask_ids_batch.to(device)\n",
    "            pos_ids_batch = pos_ids_batch.to(device)\n",
    "            vms_batch = vms_batch.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    loss, logits = model(input_ids_batch, label_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch)\n",
    "                except:\n",
    "                    print(input_ids_batch)\n",
    "                    print(input_ids_batch.size())\n",
    "                    print(vms_batch)\n",
    "                    print(vms_batch.size())\n",
    "\n",
    "            logits = nn.Softmax(dim=1)(logits)\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            gold = label_ids_batch\n",
    "#             for j in range(pred.size()[0]):\n",
    "#                 confusion[pred[j], gold[j]] += 1\n",
    "            correct += torch.sum(pred == gold).item()\n",
    "\n",
    "#         if is_test:\n",
    "#             print(\"Confusion matrix:\")\n",
    "#             print(confusion)\n",
    "#             print(\"Report precision, recall, and f1:\")\n",
    "\n",
    "#         for i in range(confusion.size()[0]):\n",
    "#             p = confusion[i,i].item()/confusion[i,:].sum().item()\n",
    "#             r = confusion[i,i].item()/confusion[:,i].sum().item()\n",
    "#             f1 = 2*p*r / (p+r)\n",
    "#             if i == 1:\n",
    "#                 label_1_f1 = f1\n",
    "#             print(\"Label {}: {:.3f}, {:.3f}, {:.3f}\".format(i,p,r,f1))\n",
    "        print(\"Acc. (Correct/Total): {:.4f} ({}/{}) \".format(correct/len(dataset), correct, len(dataset)))\n",
    "        if metrics == 'Acc':\n",
    "            return correct/len(dataset)\n",
    "        elif metrics == 'f1':\n",
    "            return label_1_f1\n",
    "        else:\n",
    "            return correct/len(dataset)\n",
    "    else:\n",
    "        print('mrr')\n",
    "        for i, (input_ids_batch, label_ids_batch,  mask_ids_batch, pos_ids_batch, vms_batch) in enumerate(batch_loader(batch_size, input_ids, label_ids, mask_ids, pos_ids, vms)):\n",
    "\n",
    "            vms_batch = torch.LongTensor(vms_batch)\n",
    "\n",
    "            input_ids_batch = input_ids_batch.to(device)\n",
    "            label_ids_batch = label_ids_batch.to(device)\n",
    "            mask_ids_batch = mask_ids_batch.to(device)\n",
    "            pos_ids_batch = pos_ids_batch.to(device)\n",
    "            vms_batch = vms_batch.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                loss, logits = model(input_ids_batch, label_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch)\n",
    "            logits = nn.Softmax(dim=1)(logits)\n",
    "            if i == 0:\n",
    "                logits_all=logits\n",
    "            if i >= 1:\n",
    "                logits_all=torch.cat((logits_all,logits),0)\n",
    "\n",
    "        order = -1\n",
    "        gold = []\n",
    "        for i in range(len(dataset)):\n",
    "            qid = dataset[i][-1]\n",
    "            label = dataset[i][1]\n",
    "            if qid == order: \n",
    "                j += 1\n",
    "                if label == 1:\n",
    "                    gold.append((qid,j))\n",
    "            else:\n",
    "                order = qid\n",
    "                j = 0\n",
    "                if label == 1:\n",
    "                    gold.append((qid,j))\n",
    "\n",
    "        label_order = []\n",
    "        order = -1\n",
    "        for i in range(len(gold)):\n",
    "            if gold[i][0] == order:\n",
    "                templist.append(gold[i][1])\n",
    "            elif gold[i][0] != order:\n",
    "                order=gold[i][0]\n",
    "                if i > 0:\n",
    "                    label_order.append(templist)\n",
    "                templist = []\n",
    "                templist.append(gold[i][1])\n",
    "        label_order.append(templist)\n",
    "\n",
    "        order = -1\n",
    "        score_list = []\n",
    "        for i in range(len(logits_all)):\n",
    "            score = float(logits_all[i][1])\n",
    "            qid=int(dataset[i][-1])\n",
    "            if qid == order:\n",
    "                templist.append(score)\n",
    "            else:\n",
    "                order = qid\n",
    "                if i > 0:\n",
    "                    score_list.append(templist)\n",
    "                templist = []\n",
    "                templist.append(score)\n",
    "        score_list.append(templist)\n",
    "\n",
    "        rank = []\n",
    "        pred = []\n",
    "        print(len(score_list))\n",
    "        print(len(label_order))\n",
    "        for i in range(len(score_list)):\n",
    "            if len(label_order[i])==1:\n",
    "                if label_order[i][0] < len(score_list[i]):\n",
    "                    true_score = score_list[i][label_order[i][0]]\n",
    "                    score_list[i].sort(reverse=True)\n",
    "                    for j in range(len(score_list[i])):\n",
    "                        if score_list[i][j] == true_score:\n",
    "                            rank.append(1 / (j + 1))\n",
    "                else:\n",
    "                    rank.append(0)\n",
    "\n",
    "            else:\n",
    "                true_rank = len(score_list[i])\n",
    "                for k in range(len(label_order[i])):\n",
    "                    if label_order[i][k] < len(score_list[i]):\n",
    "                        true_score = score_list[i][label_order[i][k]]\n",
    "                        temp = sorted(score_list[i],reverse=True)\n",
    "                        for j in range(len(temp)):\n",
    "                            if temp[j] == true_score:\n",
    "                                if j < true_rank:\n",
    "                                    true_rank = j\n",
    "                if true_rank < len(score_list[i]):\n",
    "                    rank.append(1 / (true_rank + 1))\n",
    "                else:\n",
    "                    rank.append(0)\n",
    "        MRR = sum(rank) / len(rank)\n",
    "        print(\"MRR\", MRR)\n",
    "        return MRR\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training.\n",
      "Loading sentences from ./huawei_train.csv\n",
      "There are 5324 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/5324\n",
      "Shuffling dataset\n"
     ]
    }
   ],
   "source": [
    "# Training phase.\n",
    "print(\"Start training.\")\n",
    "trainset = read_dataset(args.train_path, workers_num=args.workers_num)\n",
    "print(\"Shuffling dataset\")\n",
    "random.shuffle(trainset)\n",
    "instances_num = len(trainset)\n",
    "batch_size = args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trans data to tensor.\n",
      "input_ids\n",
      "label_ids\n",
      "mask_ids\n",
      "pos_ids\n",
      "vms\n",
      "Batch size:  30\n",
      "The number of training instances: 5324\n"
     ]
    }
   ],
   "source": [
    "print(\"Trans data to tensor.\")\n",
    "print(\"input_ids\")\n",
    "input_ids = torch.LongTensor([example[0] for example in trainset])\n",
    "print(\"label_ids\")\n",
    "label_ids = torch.LongTensor([example[1] for example in trainset])\n",
    "print(\"mask_ids\")\n",
    "mask_ids = torch.LongTensor([example[2] for example in trainset])\n",
    "print(\"pos_ids\")\n",
    "pos_ids = torch.LongTensor([example[3] for example in trainset])\n",
    "print(\"vms\")\n",
    "vms = [example[4] for example in trainset]\n",
    "\n",
    "train_steps = int(instances_num * args.epochs_num / batch_size) + 1\n",
    "\n",
    "print(\"Batch size: \", batch_size)\n",
    "print(\"The number of training instances:\", instances_num)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
    "]\n",
    "optimizer = BertAdam(optimizer_grouped_parameters, lr=args.learning_rate, warmup=args.warmup, t_total=train_steps)\n",
    "\n",
    "total_loss = 0.\n",
    "result = 0.0\n",
    "best_result = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/ed3203/K-BERT\" target=\"_blank\">https://app.wandb.ai/ed3203/K-BERT</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/ed3203/K-BERT/runs/c6rsu475\" target=\"_blank\">https://app.wandb.ai/ed3203/K-BERT/runs/c6rsu475</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "W&B Run: https://app.wandb.ai/ed3203/K-BERT/runs/c6rsu475"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(config=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<wandb.wandb_torch.TorchGraph at 0x7f530074a2b0>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "/opt/conda/conda-bld/pytorch_1587428270644/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch id: 1, Training steps: 100, Avg loss: 5.559\n",
      "Start evaluation on dev dataset.\n",
      "Loading sentences from ./huawei_test.csv\n",
      "There are 2190 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/2190\n",
      "not MMR\n",
      "Acc. (Correct/Total): 0.0785 (172/2190) \n",
      "Start evaluation on test dataset.\n",
      "Loading sentences from ./huawei_test.csv\n",
      "There are 2190 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/2190\n",
      "The number of evaluation instances:  2190\n",
      "not MMR\n",
      "Acc. (Correct/Total): 0.0785 (172/2190) \n",
      "Epoch id: 2, Training steps: 100, Avg loss: 9.062\n",
      "Start evaluation on dev dataset.\n",
      "Loading sentences from ./huawei_test.csv\n",
      "There are 2190 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/2190\n",
      "not MMR\n",
      "Acc. (Correct/Total): 0.2429 (532/2190) \n",
      "Start evaluation on test dataset.\n",
      "Loading sentences from ./huawei_test.csv\n",
      "There are 2190 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/2190\n",
      "The number of evaluation instances:  2190\n",
      "not MMR\n",
      "Acc. (Correct/Total): 0.2429 (532/2190) \n",
      "Epoch id: 3, Training steps: 100, Avg loss: 7.442\n",
      "Start evaluation on dev dataset.\n",
      "Loading sentences from ./huawei_test.csv\n",
      "There are 2190 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/2190\n",
      "not MMR\n",
      "Acc. (Correct/Total): 0.4493 (984/2190) \n",
      "Start evaluation on test dataset.\n",
      "Loading sentences from ./huawei_test.csv\n",
      "There are 2190 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/2190\n",
      "The number of evaluation instances:  2190\n",
      "not MMR\n",
      "Acc. (Correct/Total): 0.4493 (984/2190) \n",
      "Epoch id: 4, Training steps: 100, Avg loss: 5.568\n",
      "Start evaluation on dev dataset.\n",
      "Loading sentences from ./huawei_test.csv\n",
      "There are 2190 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/2190\n",
      "not MMR\n",
      "Acc. (Correct/Total): 0.5548 (1215/2190) \n",
      "Start evaluation on test dataset.\n",
      "Loading sentences from ./huawei_test.csv\n",
      "There are 2190 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/2190\n",
      "The number of evaluation instances:  2190\n",
      "not MMR\n",
      "Acc. (Correct/Total): 0.5548 (1215/2190) \n",
      "Epoch id: 5, Training steps: 100, Avg loss: 4.219\n",
      "Start evaluation on dev dataset.\n",
      "Loading sentences from ./huawei_test.csv\n",
      "There are 2190 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/2190\n",
      "not MMR\n",
      "Acc. (Correct/Total): 0.5977 (1309/2190) \n",
      "Start evaluation on test dataset.\n",
      "Loading sentences from ./huawei_test.csv\n",
      "There are 2190 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/2190\n",
      "The number of evaluation instances:  2190\n",
      "not MMR\n",
      "Acc. (Correct/Total): 0.5977 (1309/2190) \n",
      "Epoch id: 6, Training steps: 100, Avg loss: 3.345\n",
      "Start evaluation on dev dataset.\n",
      "Loading sentences from ./huawei_test.csv\n",
      "There are 2190 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/2190\n",
      "not MMR\n",
      "Acc. (Correct/Total): 0.6416 (1405/2190) \n",
      "Start evaluation on test dataset.\n",
      "Loading sentences from ./huawei_test.csv\n",
      "There are 2190 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/2190\n",
      "The number of evaluation instances:  2190\n",
      "not MMR\n",
      "Acc. (Correct/Total): 0.6416 (1405/2190) \n",
      "Epoch id: 7, Training steps: 100, Avg loss: 2.732\n",
      "Start evaluation on dev dataset.\n",
      "Loading sentences from ./huawei_test.csv\n",
      "There are 2190 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/2190\n",
      "not MMR\n",
      "Acc. (Correct/Total): 0.6808 (1491/2190) \n",
      "Start evaluation on test dataset.\n",
      "Loading sentences from ./huawei_test.csv\n",
      "There are 2190 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/2190\n",
      "The number of evaluation instances:  2190\n",
      "not MMR\n",
      "Acc. (Correct/Total): 0.6808 (1491/2190) \n",
      "Epoch id: 8, Training steps: 100, Avg loss: 2.276\n",
      "Start evaluation on dev dataset.\n",
      "Loading sentences from ./huawei_test.csv\n",
      "There are 2190 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/2190\n",
      "not MMR\n",
      "Acc. (Correct/Total): 0.7160 (1568/2190) \n",
      "Start evaluation on test dataset.\n",
      "Loading sentences from ./huawei_test.csv\n",
      "There are 2190 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/2190\n",
      "The number of evaluation instances:  2190\n",
      "not MMR\n",
      "Acc. (Correct/Total): 0.7160 (1568/2190) \n",
      "Epoch id: 9, Training steps: 100, Avg loss: 1.896\n",
      "Start evaluation on dev dataset.\n",
      "Loading sentences from ./huawei_test.csv\n",
      "There are 2190 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/2190\n",
      "not MMR\n",
      "Acc. (Correct/Total): 0.7215 (1580/2190) \n",
      "Start evaluation on test dataset.\n",
      "Loading sentences from ./huawei_test.csv\n",
      "There are 2190 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/2190\n",
      "The number of evaluation instances:  2190\n",
      "not MMR\n",
      "Acc. (Correct/Total): 0.7215 (1580/2190) \n",
      "Epoch id: 10, Training steps: 100, Avg loss: 1.595\n",
      "Start evaluation on dev dataset.\n",
      "Loading sentences from ./huawei_test.csv\n",
      "There are 2190 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/2190\n",
      "not MMR\n",
      "Acc. (Correct/Total): 0.7374 (1615/2190) \n",
      "Start evaluation on test dataset.\n",
      "Loading sentences from ./huawei_test.csv\n",
      "There are 2190 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/2190\n",
      "The number of evaluation instances:  2190\n",
      "not MMR\n",
      "Acc. (Correct/Total): 0.7374 (1615/2190) \n",
      "Epoch id: 11, Training steps: 100, Avg loss: 1.338\n",
      "Start evaluation on dev dataset.\n",
      "Loading sentences from ./huawei_test.csv\n",
      "There are 2190 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/2190\n",
      "not MMR\n",
      "Acc. (Correct/Total): 0.7466 (1635/2190) \n",
      "Start evaluation on test dataset.\n",
      "Loading sentences from ./huawei_test.csv\n",
      "There are 2190 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/2190\n",
      "The number of evaluation instances:  2190\n",
      "not MMR\n",
      "Acc. (Correct/Total): 0.7466 (1635/2190) \n",
      "Epoch id: 12, Training steps: 100, Avg loss: 1.134\n",
      "Start evaluation on dev dataset.\n",
      "Loading sentences from ./huawei_test.csv\n",
      "There are 2190 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/2190\n",
      "not MMR\n",
      "Acc. (Correct/Total): 0.7639 (1673/2190) \n",
      "Start evaluation on test dataset.\n",
      "Loading sentences from ./huawei_test.csv\n",
      "There are 2190 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/2190\n",
      "The number of evaluation instances:  2190\n",
      "not MMR\n",
      "Acc. (Correct/Total): 0.7639 (1673/2190) \n",
      "Epoch id: 13, Training steps: 100, Avg loss: 0.967\n",
      "Start evaluation on dev dataset.\n",
      "Loading sentences from ./huawei_test.csv\n",
      "There are 2190 sentence in total. We use 1 processes to inject knowledge into sentences.\n",
      "Progress of process 0: 0/2190\n",
      "not MMR\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-68b3a80ec7ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Start evaluation on dev dataset.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"acc\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-21f2f4168685>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(args, is_test, metrics)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m#             for j in range(pred.size()[0]):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m#                 confusion[pred[j], gold[j]] += 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m#         if is_test:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(1, args.epochs_num+1):\n",
    "    model.train()\n",
    "    for i, (input_ids_batch, label_ids_batch, mask_ids_batch, pos_ids_batch, vms_batch) in enumerate(batch_loader(batch_size, input_ids, label_ids, mask_ids, pos_ids, vms)):\n",
    "        model.zero_grad()\n",
    "\n",
    "        vms_batch = torch.LongTensor(vms_batch)\n",
    "\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        label_ids_batch = label_ids_batch.to(device)\n",
    "        mask_ids_batch = mask_ids_batch.to(device)\n",
    "        pos_ids_batch = pos_ids_batch.to(device)\n",
    "        vms_batch = vms_batch.to(device)\n",
    "\n",
    "        loss, _ = model(input_ids_batch, label_ids_batch, mask_ids_batch, pos=pos_ids_batch, vm=vms_batch)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            loss = torch.mean(loss)\n",
    "        total_loss += loss.item()\n",
    "        if (i + 1) % args.report_steps == 0:\n",
    "            ave_loss = total_loss / args.report_steps\n",
    "            print(\"Epoch id: {}, Training steps: {}, Avg loss: {:.3f}\".format(epoch, i+1, ave_loss))\n",
    "            sys.stdout.flush()\n",
    "            total_loss = 0.\n",
    "            wandb.log({\"train_loss\":ave_loss})\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Start evaluation on dev dataset.\")\n",
    "    result = evaluate(args, False)\n",
    "    wandb.log({\"acc\": result})\n",
    "    if result > best_result:\n",
    "        best_result = result\n",
    "        save_model(model, args.output_model_path)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    print(\"Start evaluation on test dataset.\")\n",
    "    evaluate(args, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluate(args, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg.entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(kg.entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in set(kg.entities):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in kg.segment_vocab[:200]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation phase.\n",
    "print(\"Final evaluation on the test dataset.\")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model.module.load_state_dict(torch.load(args.output_model_path))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(args.output_model_path))\n",
    "evaluate(args, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(args.train_path, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in kg.entities:\n",
    "    if np.any([True for e in ['歌手达明一派', '歌手达明', '性质技术用语'] if e in i]):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.text_a[:3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg.lookup_table.get(token, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kg.entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = pkuseg.pkuseg(model_name=\"default\", postag=True)#, user_dict=self.segment_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.text_a[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "How do I appeal to change the password of Huawei account? My previous mobile phone was disabled and I forgot my password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with pos tag\n",
    "tok = pkuseg.pkuseg(model_name=\"default\", postag=True)#, user_dict=self.segment_vocab)\n",
    "p= tok.cut('怎么申诉更改华为账号密码呀，我之前的手机停用了，密码也忘了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ['zenme how','shensu appeal','genggai change','huawei huawei','zhanghao account number','mima password','ya yeah',',','wo I','zhiqian prior to','of','shouji cellphone','ting young deactivate','le off/past tense',',','mima password','ye and also','wang forget','le past tense']\n",
    "for c, i in enumerate(p):\n",
    "    print(i, t[c])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "R - Adverb\n",
    "f - ?\n",
    "v - verb\n",
    "y - ?\n",
    "w - ? delimiter?\n",
    "n - noun\n",
    "u -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without pos tag\n",
    "tok = pkuseg.pkuseg(model_name=\"default\", postag=False)#, user_dict=self.segment_vocab)\n",
    "tok.cut('怎么申诉更改华为账号密码呀，我之前的手机停用了，密码也忘了')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENGLISH DBPEDIA INSPECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('./brain/kgs/english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.read_csv('./brain/kgs/english/Device.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.URI.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.columns.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.columns.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.split('resource/')[1] for i in db.manufacturer.unique() if type(i)==str and 'resource' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "db.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in db['rdf-schema#label'].values[:]:\n",
    "    i=str(i).lower()\n",
    "    if 'huawei' in i:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in db['URI'].values[:]:\n",
    "    i=str(i).lower()\n",
    "    if 'huawei' in i:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "drop = ['acceleration','author_label','author','automobileModel','background','blockAlloy_label','blockAlloy','co2Emission','compressionRatio',\n",
    "       'computingPlatform_label','configuration','']\n",
    "for i in drop:\n",
    "    try:\n",
    "        db = db.loc[db[i].isnull()]\n",
    "        db.drop(labels=[i], axis=1, inplace=True, errors='ignore')\n",
    "    except AttributeError as e:\n",
    "        print(i,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in db.columns:\n",
    "    n = db[c].nunique()\n",
    "    if n <200:\n",
    "        print(c, n, db[c].unique())\n",
    "    else:\n",
    "        print(c,n)\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cndbpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.kg_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('./brain/kgs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = []\n",
    "pa = []\n",
    "oa = []\n",
    "\n",
    "with open(spo_path, 'r', encoding='utf-8') as f:\n",
    "    for line in tqdm(f):\n",
    "        try:\n",
    "            subj, pred, obje = line.strip().split(\"\\t\")    \n",
    "            sa.append(subj)\n",
    "            pa.append(pred)\n",
    "            oa.append(obje)\n",
    "        except:\n",
    "            pass\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(sa)), len(set(pa)), len(set(oa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(pa).value_counts().head(n=30).index.tolist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##MOST FREQUENT PREDICATES\n",
    "'Serialize website',\n",
    "  'Author',\n",
    "  'Book title',\n",
    "  'Publisher',\n",
    "  'name of software',\n",
    "  'Serialization platform',\n",
    "  'Chinese name',\n",
    "  'Types of',\n",
    "  'category',\n",
    "  'game type',\n",
    "  'Title',\n",
    "  'district belong to',\n",
    "  'Geographic location',\n",
    "  'Application Name',\n",
    "  'Restaurant name',\n",
    "  'nature',\n",
    "  'Subclass',\n",
    "  'company name',\n",
    "  'Property name',\n",
    "  'Literary genre',\n",
    "  'Language',\n",
    "  'nickname',\n",
    "  'Yamen',\n",
    "  'Subfamily',\n",
    "  'Production area',\n",
    "  'Short name',\n",
    "  'Alias',\n",
    "  'Attributes',\n",
    "  'Suborder',\n",
    "  'location'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## EXAMPLE SPO\n",
    "'Red food\\tsuitable people\\tall people\\n',\n",
    "  'Perimenopausal syndrome\\t alias\\t menopausal syndrome\\n',\n",
    "  'Share split\\tdivided into\\tsocial public shares\\n',\n",
    "  'Baidu Encyclopedia\\tCore User\\tEncyclopedia Tadpole Group\\n',\n",
    "  'Baidu Encyclopedia\\tcampus user\\tBaidu Encyclopedia campus ambassador\\n',\n",
    "  'Five Doctor Songs\\tCity\\t Tai'an City, Shandong Province\\n',\n",
    "  'Storm Audio\\tSoftware Language\\tSimplified Chinese\\n',\n",
    "  'Assembly language\\tdiscipline\\tsoftware engineering\\n',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(oa).value_counts().head(n=30)#.index.tolist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Object frequency\n",
    "Starting Point Chinese Network 376776\n",
    "Jinjiang Literature City 138016\n",
    "Jinjiang Literature Network 123493\n",
    "Fiction Reading Net 69198\n",
    "Red Sleeve Tianxiang Net 67017\n",
    "Yunzhong Book City 58611\n",
    "Simplified Chinese 50236\n",
    "Across Chinese Network 45117\n",
    "Xiaoxiang Academy 25866\n",
    "Online Novel 22580\n",
    "Magic Sword Book League 18981\n",
    "Mainland China 17078\n",
    "Sina reading 15466\n",
    "Vertebrate subphylum 14752\n",
    "Puzzle game 13926\n",
    "Liancheng Reading 13605\n",
    "Dress Up Games 13417\n",
    "Suitable for all ages 9306\n",
    "Casual games 8791\n",
    "Feiluo Novel Net 8736\n",
    "Original perianthia 8351\n",
    "Residence 8319\n",
    "Winged subclass 7417\n",
    "Public works 7399\n",
    "Hexapod Asian Gate 7086\n",
    "Subtropical monsoon climate 7084\n",
    "Hong Kong, China 6929\n",
    "Five-word poem 6637\n",
    "Seven-word poem 6534\n",
    "New novel bar 6495"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spo_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(spo_path, 'r', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banned_pred = ['连载网站', '作者', '书名', '出版社']\n",
    "banned_obj = ['起点中文网','晋江文学城','晋江文学网','小说阅读网','红袖添香网','云中书城']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n= open('./brain/kgs/short_CnDbpedia.spo', 'w', encoding='utf-8')\n",
    "# spo_path=f'./brain/kgs/{args.kg_name}.spo'\n",
    "# f= open(spo_path, 'r', encoding='utf-8')\n",
    "# for line in tqdm(f):\n",
    "#     try:\n",
    "#         s, p, o = line.strip().split(\"\\t\") \n",
    "#         if p not in banned_pred and o not in banned_obj:\n",
    "#             n.write(f'{s}\\t{p}\\t{o}\\n')\n",
    "# #         print(s,p,o)\n",
    "#     except:\n",
    "#         pass\n",
    "# #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = []\n",
    "pa = []\n",
    "oa = []\n",
    "\n",
    "f= open('./brain/kgs/short_CnDbpedia.spo', 'r', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    try:\n",
    "        subj, pred, obje = line.strip().split(\"\\t\")    \n",
    "        sa.append(subj)\n",
    "        pa.append(pred)\n",
    "        oa.append(obje)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(oa).value_counts().head(n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(pa).value_counts().head(n=30).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
